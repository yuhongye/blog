这是对google的《HyperLogLog in Practice: Algorithm ENgineering of a State of The Art Candinality Estimation Algorithm》的阅读笔记. 首先会介绍LinearCounting和HyperLogLog算法，然后是对论文中各章节的总结.

#0.LinearCounting, LogLogCount  and HyperLogLog

## 1 LinearCounting, 空间复杂度O(N)
设一哈希函数H，其哈希空间为m(最小值0，最大值m-1), 并且服从均匀分布。使用一个长度为m的bitmap, 初始值都为0，对于待预估的集合，使用H得到一个值k, bitmap(k) = 1；遍历完整个集合后，得到bitmap中值为0的个数为u, 则n=-mlog(${u} \over {m}$)​为基数的一个估计值，且为最大似然估计。

其空间复杂度为O(Nmax)，大约是使用bitmap的十分之一，该算法的空间复杂度较高，但在元素数据较少的时候表现优异，用来弥补HyperLogLog在元素数较少时预估偏大的缺陷。

## 2 LogLogCounting, 空间复杂度O(log2(log2(N))))
LogLogCounting空间复杂度为O(log2(log2(N)))), 使得通过KB级内存预估数亿级别的基数成为可能，目前在处理大数据时基数计算问题时，所采用的算法基本是LLC或其变种。

###  均匀随机化
首先需要选取一个哈希函数H应用于所有元素，然后对哈希值进行基数估计，H必须满足如下条件:

1. H的结果具有很好的随机性，也就是说无论原始集合元素的值分布如何，其哈希结果的值几乎服从均匀分布（D.Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对服从均匀分布，但是很多哈希函数可以生成近乎服从均匀分布的结果）
2. H的碰撞几乎可以忽略不计
3. H的结果是固定长度的, 设为L. 这里有一个疑问: 目前主流的hash函数都会把所有位数占满吗?

### 思想来源
设w是待估集合中某个元素经过hash后得到的值，w可以看做一个固定长度为L的bit串，每个bit的编码分别为1, 2, ...L, 设p(w)为w的bit串中第一次出现1的位置，显然`1<=p(w)<=L`。如果我们遍历集合的全部元素后，取$p_{max}$为所有p(w)中最大的那一个，可以把n=${2^{p_{max}}}$作为基数的一个粗糙估计，根据伯努利过程，可以得到下面两条解释:

    1. 如果n 远远小于 ${2^{p_{max}}}$, 那么我们得到pmax为当前值的概率几乎为0
    2. 如果n 远远大于 ${2^{p_{max}}}$, 那么我们得到pmax为当前值的概率也几乎为0

因此可以把${2^{p_{max}}}$作为基数的一个粗糙估计

### 分桶平均
如果直接使用上面的单一估计量进行基数估计，会由于偶然性导致较大的误差，LLC采用了分桶平均的思想来消除误差。具体来说就是:
    
    1. 将哈希空间分成m份，每份称之为一个桶(bucket)，哈希值w的前p位用来标识桶，则m=2^p
    2. 剩下的L-p位作为基数估计的比特串，得到每个桶的pmax
    3. pmax = 全部桶的pmax的均值, n = 2^pmax
    4. 偏差修正，2^pmax是一个粗糙估计，需要修正，修正后的结果：n = am * 2^pmax, 其中am=xxx
    5. 误差分析，LLC的错误率为: 1.30 / ()







